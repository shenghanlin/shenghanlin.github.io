<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://awsaf49.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://awsaf49.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-09-10T08:39:10+00:00</updated><id>https://awsaf49.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">LLM Science Exam</title><link href="https://awsaf49.github.io/blog/2023/llm-science/" rel="alternate" type="text/html" title="LLM Science Exam"/><published>2023-08-18T12:57:00+00:00</published><updated>2023-08-18T12:57:00+00:00</updated><id>https://awsaf49.github.io/blog/2023/llm-science</id><content type="html" xml:base="https://awsaf49.github.io/blog/2023/llm-science/"><![CDATA[<p>It may take a few moments for the Jupyter notebook to load. Thank you for your patience =)</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/notebook/llm-science-exam.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="kaggle"/><category term="notebook"/><category term="transformer"/><category term="nlp"/><summary type="html"><![CDATA[How to Turn a NLP Classifer into a Multiple Choice Question Model]]></summary></entry><entry><title type="html">2.5D Training - Unleashing 3D Power on a 2D Budget</title><link href="https://awsaf49.github.io/blog/2023/2.5d-training/" rel="alternate" type="text/html" title="2.5D Training - Unleashing 3D Power on a 2D Budget"/><published>2023-05-12T19:53:00+00:00</published><updated>2023-05-12T19:53:00+00:00</updated><id>https://awsaf49.github.io/blog/2023/2.5d-training</id><content type="html" xml:base="https://awsaf49.github.io/blog/2023/2.5d-training/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://storage.googleapis.com/kaggle-competitions/kaggle/27923/logos/header?t=2021-06-02-20-30-25-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://storage.googleapis.com/kaggle-competitions/kaggle/27923/logos/header?t=2021-06-02-20-30-25-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://storage.googleapis.com/kaggle-competitions/kaggle/27923/logos/header?t=2021-06-02-20-30-25-1400.webp"/> <img src="https://storage.googleapis.com/kaggle-competitions/kaggle/27923/logos/header.png?t=2021-06-02-20-30-25" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="introduction">Introduction</h2> <p>In this project I‚Äôll design a model that automatically segments the stomach and intestines in MRI scans for cancer patient care. The MRI scans originate from cancer patients who underwent 1-5 MRI scans on separate days during their radiation treatment. The algorithm I‚Äôm developing relies on a dataset of these scans, aiming to innovate deep learning solutions that will improve patient outcomes. A significant aspect of this project involves a technique known as 2.5D imaging, which merges the power of 3D imaging with the simplicity and efficiency of 2D resources.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://i.ibb.co/sgsPf4v/Capture-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://i.ibb.co/sgsPf4v/Capture-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://i.ibb.co/sgsPf4v/Capture-1400.webp"/> <img src="https://i.ibb.co/sgsPf4v/Capture.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> MRI scans of 2D and 2.5 images </div> <h2 id="dataset-description">Dataset Description</h2> <p>The competition utilizes RLE-encoded masks for training annotations, with images presented in 16-bit grayscale PNG format. Each case is represented by multiple sets of scan slices, with some cases split by time, others by case, urging the development of a model that generalizes to both partially and wholly unseen cases.</p> <h2 id="the-magic-of-25d-imaging">The Magic of 2.5D Imaging</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://i.ibb.co/KKtZ7Gn/Picture1-3d-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://i.ibb.co/KKtZ7Gn/Picture1-3d-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://i.ibb.co/KKtZ7Gn/Picture1-3d-1400.webp"/> <img src="https://i.ibb.co/KKtZ7Gn/Picture1-3d.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 2.5D image creation. </div> <p>While 2D training on MRI scans is relatively straightforward, the depth information in the MRI slices opens up a whole new world of possibilities. By stacking consecutive slices, we can form what appears to be a 3D volume. However, I dub this process 2.5D imaging, because we train these 3D-like images as if they were 2D.</p> <p>In a regular 2D training scenario, such as with RGB images, we pass a 3D tensor (e.g., <code class="language-plaintext highlighter-rouge">[None, channel, height, width]</code>) to a model. In PyTorch, the last two dimensions represent the spatial aspects (height &amp; width), and the first one is the <strong>channel</strong> dimension. But in the case of MRI images, where channel information is absent, we can use that dimension to <strong>stack multiple MRI scans as channels and train them as 2D images</strong>.</p> <p>This 2.5D method brings with it a raft of benefits over traditional 3D training:</p> <ul> <li>Lower GPU/memory cost</li> <li>Simplified pipeline</li> <li>Easier augmentation</li> <li>More straightforward inference</li> <li>Wide availability of open-source models</li> </ul> <p>The result mirrors a <strong>3D movie scene in the theater</strong>, as illustrated above. The advantages of this approach manifest in the impressive lb &amp; cv scores achieved, probably owing to the extra <strong>depth information</strong> obtained through stacking multiple consecutive slices in channels.</p> <p>In my approach, to maintain aspect ratios and prevent data loss, I have opted for padding instead of resizing images. Hence, the training image size stands at <code class="language-plaintext highlighter-rouge">320x384</code>. You can explore this further in my <a href="https://www.kaggle.com/awsaf49/uwmgi-2-5d-train-pytorch/">notebook</a>.</p> <h2 id="how-to-implement-25d-imaging">How to Implement 2.5D Imaging</h2> <h3 id="utility-functions">Utility Functions</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load Image
</span><span class="k">def</span> <span class="nf">load_img</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">IMG_SIZE</span><span class="p">):</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">imread</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">IMREAD_UNCHANGED</span><span class="p">)</span>
    <span class="n">shape0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">img</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">resize</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="nf">any</span><span class="p">(</span><span class="n">shape0</span><span class="o">!=</span><span class="n">resize</span><span class="p">):</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="n">resize</span> <span class="o">-</span> <span class="n">shape0</span>
        <span class="n">pad0</span> <span class="o">=</span> <span class="n">diff</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">pad1</span> <span class="o">=</span> <span class="n">diff</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">pady</span> <span class="o">=</span> <span class="p">[</span><span class="n">pad0</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">pad0</span><span class="o">//</span><span class="mi">2</span> <span class="o">+</span> <span class="n">pad0</span><span class="o">%</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">padx</span> <span class="o">=</span> <span class="p">[</span><span class="n">pad1</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">pad1</span><span class="o">//</span><span class="mi">2</span> <span class="o">+</span> <span class="n">pad1</span><span class="o">%</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">pad</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="p">[</span><span class="n">pady</span><span class="p">,</span> <span class="n">padx</span><span class="p">])</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="p">.</span><span class="nf">reshape</span><span class="p">((</span><span class="o">*</span><span class="n">resize</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">img</span>

<span class="c1"># Load Mask with .npy format
</span><span class="k">def</span> <span class="nf">load_msk</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">IMG_SIZE</span><span class="p">):</span>
    <span class="n">msk</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="n">shape0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">msk</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">resize</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="nf">any</span><span class="p">(</span><span class="n">shape0</span><span class="o">!=</span><span class="n">resize</span><span class="p">):</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="n">resize</span> <span class="o">-</span> <span class="n">shape0</span>
        <span class="n">pad0</span> <span class="o">=</span> <span class="n">diff</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">pad1</span> <span class="o">=</span> <span class="n">diff</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">pady</span> <span class="o">=</span> <span class="p">[</span><span class="n">pad0</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">pad0</span><span class="o">//</span><span class="mi">2</span> <span class="o">+</span> <span class="n">pad0</span><span class="o">%</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">padx</span> <span class="o">=</span> <span class="p">[</span><span class="n">pad1</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">pad1</span><span class="o">//</span><span class="mi">2</span> <span class="o">+</span> <span class="n">pad1</span><span class="o">%</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">msk</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">pad</span><span class="p">(</span><span class="n">msk</span><span class="p">,</span> <span class="p">[</span><span class="n">pady</span><span class="p">,</span> <span class="n">padx</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]])</span>
        <span class="n">msk</span> <span class="o">=</span> <span class="n">msk</span><span class="p">.</span><span class="nf">reshape</span><span class="p">((</span><span class="o">*</span><span class="n">resize</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">msk</span>

<span class="c1"># Load multiple images
</span><span class="k">def</span> <span class="nf">load_imgs</span><span class="p">(</span><span class="n">img_paths</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">IMG_SIZE</span><span class="p">):</span>
    <span class="n">imgs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="o">*</span><span class="n">size</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">img_paths</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">uint16</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">img_path</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">img_paths</span><span class="p">):</span>
        <span class="n">img</span> <span class="o">=</span> <span class="nf">load_img</span><span class="p">(</span><span class="n">img_path</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>
        <span class="n">imgs</span><span class="p">[...,</span> <span class="n">i</span><span class="p">]</span><span class="o">+=</span><span class="n">img</span>
    <span class="k">return</span> <span class="n">imgs</span>

</code></pre></div></div> <h3 id="extract-meta-data">Extract Meta Data</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">../input/uwmgi-mask-dataset/train.csv</span><span class="sh">'</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">segmentation</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">segmentation</span><span class="p">.</span><span class="nf">fillna</span><span class="p">(</span><span class="sh">''</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">rle_len</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">segmentation</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="nb">len</span><span class="p">)</span> <span class="c1"># length of each rle mask
</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">mask_path</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">mask_path</span><span class="p">.</span><span class="nb">str</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s">/png/</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">/np</span><span class="sh">'</span><span class="p">).</span><span class="nb">str</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s">.png</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">.npy</span><span class="sh">'</span><span class="p">)</span>

<span class="n">df2</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">groupby</span><span class="p">([</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">])[</span><span class="sh">'</span><span class="s">segmentation</span><span class="sh">'</span><span class="p">].</span><span class="nf">agg</span><span class="p">(</span><span class="nb">list</span><span class="p">).</span><span class="nf">to_frame</span><span class="p">().</span><span class="nf">reset_index</span><span class="p">()</span> <span class="c1"># rle list of each id
</span><span class="n">df2</span> <span class="o">=</span> <span class="n">df2</span><span class="p">.</span><span class="nf">merge</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="nf">groupby</span><span class="p">([</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">])[</span><span class="sh">'</span><span class="s">rle_len</span><span class="sh">'</span><span class="p">].</span><span class="nf">agg</span><span class="p">(</span><span class="nb">sum</span><span class="p">).</span><span class="nf">to_frame</span><span class="p">().</span><span class="nf">reset_index</span><span class="p">())</span> <span class="c1"># total length of all rles of each id
</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">segmentation</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">class</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">rle_len</span><span class="sh">'</span><span class="p">])</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">groupby</span><span class="p">([</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">]).</span><span class="nf">head</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">merge</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">])</span>
<span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">empty</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">rle_len</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># empty masks
</span><span class="n">df</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span>
</code></pre></div></div> <h3 id="create-25d-images-this-is-where-the-magic-happens">Create 2.5D Images (This is where the magic happens!)</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">channels</span><span class="o">=</span><span class="mi">3</span>
<span class="n">stride</span><span class="o">=</span><span class="mi">2</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">channels</span><span class="p">):</span>
    <span class="n">df</span><span class="p">[</span><span class="sa">f</span><span class="sh">'</span><span class="s">image_path_</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="mi">02</span><span class="si">}</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">groupby</span><span class="p">([</span><span class="sh">'</span><span class="s">case</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">day</span><span class="sh">'</span><span class="p">])[</span><span class="sh">'</span><span class="s">image_path</span><span class="sh">'</span><span class="p">].</span><span class="nf">shift</span><span class="p">(</span><span class="o">-</span><span class="n">i</span><span class="o">*</span><span class="n">stride</span><span class="p">).</span><span class="nf">fillna</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="sh">"</span><span class="s">ffill</span><span class="sh">"</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">image_paths</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="sa">f</span><span class="sh">'</span><span class="s">image_path_</span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="mi">02</span><span class="n">d</span><span class="si">}</span><span class="sh">'</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">channels</span><span class="p">)]].</span><span class="n">values</span><span class="p">.</span><span class="nf">tolist</span><span class="p">()</span>
<span class="n">df</span><span class="p">.</span><span class="n">image_paths</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div> <h3 id="display-25d-images">Display 2.5D Images</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">idx</span> <span class="o">=</span> <span class="mi">40</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">img</span> <span class="o">=</span> <span class="nf">load_img</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">image_path</span><span class="p">[</span><span class="n">idx</span><span class="p">]).</span><span class="nf">astype</span><span class="p">(</span><span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">)</span>
<span class="n">img</span><span class="o">/=</span><span class="n">img</span><span class="p">.</span><span class="nf">max</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">);</span> <span class="n">plt</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>


<span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">imgs</span> <span class="o">=</span> <span class="nf">load_imgs</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">image_paths</span><span class="p">[</span><span class="n">idx</span><span class="p">]).</span><span class="nf">astype</span><span class="p">(</span><span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">)</span>
<span class="n">imgs</span><span class="o">/=</span><span class="n">imgs</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">imgs</span><span class="p">);</span> <span class="n">plt</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p>Output:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2.5d-demo-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2.5d-demo-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2.5d-demo-1400.webp"/> <img src="/assets/img/2.5d-demo.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 2D vs 2.5D image. </div> <h2 id="weights-and-biases-visualization">Weights and Biases Visualization</h2> <p><img src="" alt="WandB Visualization"/></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://i.ibb.co/KjVyR3M/wandb-ss-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://i.ibb.co/KjVyR3M/wandb-ss-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://i.ibb.co/KjVyR3M/wandb-ss-1400.webp"/> <img src="https://i.ibb.co/KjVyR3M/wandb-ss.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Through Weights and Biases visualization, you can track all the experiments <a href="https://wandb.ai/awsaf49/uw-maddison-gi-tract">here</a>.</p> <h2 id="access-the-project">Access the Project</h2> <p>Here are some useful links to dive deeper into the project:</p> <ul> <li>Training Notebook: <a href="https://www.kaggle.com/awsaf49/uwmgi-2-5d-train-pytorch/">UWMGI: 2.5D [Train] [PyTorch]</a></li> <li>Inference Notebook: <a href="https://www.kaggle.com/awsaf49/uwmgi-2-5d-infer-pytorch/">UWMGI: 2.5D [Infer] [PyTorch]</a> (LB: 0.86+)</li> <li>Data: <a href="https://www.kaggle.com/code/awsaf49/uwmgi-2-5d-stride-2-data/">UWMGI: 2.5D stride=2 Data</a></li> <li>Dataset: <a href="https://www.kaggle.com/datasets/awsaf49/uwmgi-25d-stride2-dataset">UWMGI: 2.5D stride=2 Dataset</a></li> </ul> ]]></content><author><name></name></author><category term="kaggle"/><category term="cnn"/><category term="transformer"/><summary type="html"><![CDATA[How to get 3D data performance with 2D models]]></summary></entry><entry><title type="html">CNN vs Transformer</title><link href="https://awsaf49.github.io/blog/2023/cnn-vs-transformer/" rel="alternate" type="text/html" title="CNN vs Transformer"/><published>2023-05-12T19:53:00+00:00</published><updated>2023-05-12T19:53:00+00:00</updated><id>https://awsaf49.github.io/blog/2023/cnn-vs-transformer</id><content type="html" xml:base="https://awsaf49.github.io/blog/2023/cnn-vs-transformer/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In the <a href="https://www.kaggle.com/competitions/petfinder-pawpularity-score/">PetFinder.my - Pawpularity Contest</a>, participants were challenged to predict which pets were more likely to be adopted based on their images and metadata. Initially, Convolutional Neural Networks (CNNs) showed promising results in this competition. However, as time went on, a new model called Transformer emerged and surpassed CNN by a significant margin. This raises the question: <strong>Why did the Transformer outperform CNN?</strong></p> <h2 id="why">Why??</h2> <p>When it comes to adopting a pet, humans are often drawn to pets that look appealing. However, this appeal is not solely dependent on a cute face; it considers the entire picture, including the tail, fur, surroundings, and more. Similarly, we expect a deep learning model to consider these factors when making predictions. Let‚Äôs delve into what CNN and Transformer models focus on in an image before making their decisions.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cnn_vs_transformer.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cnn_vs_transformer.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cnn_vs_transformer.gif-1400.webp"/> <img src="/assets/img/cnn_vs_transformer.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> GradCAM of CNN vs AttentionMAP of Transformer. </div> <p>From the animation above, we can observe that the CNN model (<code class="language-plaintext highlighter-rouge">EfficientNet</code>) mainly concentrates on the facial features of the image, such as the eyes, nose, and mouth. On the other hand, the Transformer model (<code class="language-plaintext highlighter-rouge">ViT</code>) pays attention to all the features of the image, including the tail, fur, surroundings, and more.</p> <h2 id="how">How??</h2> <p>Now that we understand the ‚Äúwhy,‚Äù let‚Äôs explore the <strong>‚ÄúHow‚Äù behind the Transformer‚Äôs success and the CNN‚Äôs limitations.</strong></p> <h3 id="convolutional-neural-network-cnn">Convolutional Neural Network (CNN)</h3> <p>Code:</p> <ul> <li>train: <a href="https://www.kaggle.com/awsaf49/tf-petfinder-image-tpu-train">PetFinder: CNN [TPU][Train] üê∂</a></li> <li>infer: <a href="https://www.kaggle.com/awsaf49/tf-petfinder-image-tpu-infer">PetFinder: CNN [TPU][Infer] üê∂</a></li> </ul> <blockquote> <p>CNNs are limited by their <strong>local receptive field</strong>. During the convolutional process, a CNN can only perceive information within its <code class="language-plaintext highlighter-rouge">kernel_size</code>, which means it lacks the context of regions outside that area‚Äîa limitation referred to as ‚Äúlocal-context.‚Äù Typically, the kernel size in a CNN model is set to <code class="language-plaintext highlighter-rouge">3x3</code> or <code class="language-plaintext highlighter-rouge">5x5</code>. While the receptive field of a CNN expands as the model‚Äôs depth increases, thanks to the reduction in resolution, it still remains ‚Äúshort-sighted‚Äù due to the limited receptive field.</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://149695847.v2.pressablecdn.com/wp-content/uploads/2018/01/conv-full-layer.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://149695847.v2.pressablecdn.com/wp-content/uploads/2018/01/conv-full-layer.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://149695847.v2.pressablecdn.com/wp-content/uploads/2018/01/conv-full-layer.gif-1400.webp"/> <img src="https://149695847.v2.pressablecdn.com/wp-content/uploads/2018/01/conv-full-layer.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Mechanism of CNn </div> <h3 id="transformer">Transformer</h3> <p>Code:</p> <ul> <li>train: <a href="https://www.kaggle.com/awsaf49/tf-petfinder-vit-cls-tpu-train/">PetFinder: Transformer [TPU][Train] üò∫</a></li> <li>infer: <a href="https://www.kaggle.com/awsaf49/tf-petfinder-vit-cls-tpu-infer">PetFinder: Transformer [TPU][Infer] üò∫</a></li> </ul> <blockquote> <p>On the other hand, Transformers possess a superpower known as the <strong>Global Receptive Field</strong>. This allows them to perceive the entire picture when making decisions. Transformers achieve this global awareness through a mechanism called <code class="language-plaintext highlighter-rouge">self-attention</code> which dynamically focuses on relevant information from different parts of the image enabling better understanding of the scene.</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://upload.wikimedia.org/wikipedia/commons/3/3e/Vision_Transformer.gif-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://upload.wikimedia.org/wikipedia/commons/3/3e/Vision_Transformer.gif-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://upload.wikimedia.org/wikipedia/commons/3/3e/Vision_Transformer.gif-1400.webp"/> <img src="https://upload.wikimedia.org/wikipedia/commons/3/3e/Vision_Transformer.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Mechanism of Vision Transformer </div> <blockquote class="block-tip"> <p><strong>Summary:</strong> CNNs are limited by their local-context perspective while Transformers can capture the broader context of an image, including the appealing features that contribute to a pet‚Äôs adoptability.</p> </blockquote> ]]></content><author><name></name></author><category term="kaggle"/><category term="cnn"/><category term="transformer"/><summary type="html"><![CDATA[Why use Transformer outperforms CNN??]]></summary></entry><entry><title type="html">Implementation of Probabilistic FScore</title><link href="https://awsaf49.github.io/blog/2022/pf-score/" rel="alternate" type="text/html" title="Implementation of Probabilistic FScore"/><published>2022-12-19T12:57:00+00:00</published><updated>2022-12-19T12:57:00+00:00</updated><id>https://awsaf49.github.io/blog/2022/pf-score</id><content type="html" xml:base="https://awsaf49.github.io/blog/2022/pf-score/"><![CDATA[<p>It may take a few moments for the Jupyter notebook to load. Thank you for your patience =)</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/notebook/pf_score.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="paper"/><category term="kaggle"/><category term="notebook"/><category term="metric"/><summary type="html"><![CDATA[Implement Probabilistic FScore (pF) in TensorFlow, PyTorch, Numpy]]></summary></entry><entry><title type="html">Global Context Vision Transformer (GCViT)</title><link href="https://awsaf49.github.io/blog/2022/gcvit/" rel="alternate" type="text/html" title="Global Context Vision Transformer (GCViT)"/><published>2022-08-19T12:57:00+00:00</published><updated>2022-08-19T12:57:00+00:00</updated><id>https://awsaf49.github.io/blog/2022/gcvit</id><content type="html" xml:base="https://awsaf49.github.io/blog/2022/gcvit/"><![CDATA[<p>It may take a few moments for the Jupyter notebook to load. Thank you for your patience =)</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/notebook/gcvit.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="paper"/><category term="kaggle"/><category term="notebook"/><category term="transformer"/><summary type="html"><![CDATA[Implementation & Explanation of GCViT paper with TensorFlow 2.0]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://awsaf49.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://awsaf49.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://awsaf49.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry></feed>